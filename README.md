# üõ†Ô∏è Constru√ß√£o de ETL Otimizado com PySpark

## Apache Spark

O Apache Spark √© uma plataforma de processamento de dados em grande escala e oferece ferramentas poderosas para processar grandes volumes de dados de forma distribu√≠da e eficiente.
O **PySpark √© uma interface em Python para o Apache Spark** e permite que voc√™ utilize toda a funcionalidade do Spark, como **processamento de dados em batch e streaming, machine learning (MLlib), SQL e muito mais**.

## Vantagens do PySpark

- **Escalabilidade:** Pode processar grandes volumes de dados, distribuindo tarefas entre m√∫ltiplas m√°quinas.
- **Velocidade:** Em compara√ß√£o com outras tecnologias de processamento, o Spark pode ser muito mais r√°pido devido √† sua arquitetura em mem√≥ria.
- **Flexibilidade:** Oferece APIs para processamento de dados em batch, SQL e machine learning.
- **Suporte a SQL:** Utiliza o Spark SQL para consultas SQL, o que facilita a integra√ß√£o com outras ferramentas.

---

## Construindo ETL em PySpark

Quando falamos de ETL (Extract, Transform, Load) com PySpark, o objetivo √© construir pipelines eficientes que extraem dados de fontes, fazem transforma√ß√µes e os carregam para o destino.
O processo pode ser feito utilizando **Spark DataFrames** (estrutura de dados imut√°vel e distribu√≠da que suporta opera√ß√µes em grandes volumes de dados) e **Spark SQL** para transformar os dados de forma otimizada.

## Diretrizes para otimizar o processo de ETL usando PySpark:

### 1. Extra√ß√£o (Extract)

Na fase de extra√ß√£o, os dados podem vir de diferentes fontes, como arquivos CSV, bancos de dados SQL, NoSQL, etc. No PySpark, a extra√ß√£o √© realizada atrav√©s da leitura de arquivos ou conex√µes com fontes externas.

### 2. Transforma√ß√£o (Transform)

As transforma√ß√µes no PySpark s√£o feitas com a API de DataFrame ou com SQL. O Spark permite opera√ß√µes como filtro, agrega√ß√£o, jun√ß√£o e ordena√ß√£o de dados de forma distribu√≠da e eficiente.

- **Filtros e sele√ß√µes:** Filtrar e selecionar colunas espec√≠ficas.
- **Agrega√ß√£o e agrega√ß√µes avan√ßadas:** Como contagem, soma e m√©dias.
- **Jun√ß√µes (Joins):** O Spark √© otimizado para jun√ß√µes distribu√≠das.
- **Manipula√ß√µes de dados:** Como adicionar novas colunas ou transformar dados.

### 3. Carregamento (Load)

A carga dos dados no destino pode ser feita de v√°rias formas, incluindo salvar no HDFS, S3, em bancos de dados, etc.

### 4. Otimiza√ß√µes em PySpark

- **Uso de Parquet:** O formato Parquet √© altamente otimizado para grandes volumes de dados e tem compress√£o eficiente. Quando poss√≠vel, preferir trabalhar com esse formato ao inv√©s de CSV.
- **Persist√™ncia:** Se voc√™ realizar m√∫ltiplas opera√ß√µes em um DataFrame, voc√™ pode persistir ele na mem√≥ria para evitar reprocessamentos desnecess√°rios.
- **Particionamento:** Dividir o DataFrame em parti√ß√µes para otimizar a leitura e escrita de dados.
- **Reparticionamento:** Antes de opera√ß√µes como agrega√ß√µes e joins, o reparticionamento pode melhorar a performance.
- **Filtragem de dados antes de transforma√ß√£o:** Evite carregar dados desnecess√°rios. Aplique filtros e proje√ß√µes (sele√ß√£o de colunas) o mais cedo poss√≠vel no pipeline.
- **Broadcast Join:** Se voc√™ est√° fazendo um join entre uma grande tabela e uma pequena tabela, voc√™ pode usar broadcast para a tabela pequena, o que pode melhorar a performance.
- **Persist√™ncia em disco:** Quando os dados s√£o grandes e voc√™ n√£o quer que a mem√≥ria se esgote, use disk para persistir dados intermedi√°rios.

### 5. Uso de PySpark SQL

A utiliza√ß√£o de SQL em PySpark √© poderosa e pode ser feita tanto atrav√©s da API SQL do PySpark quanto atrav√©s da execu√ß√£o de queries SQL diretamente no SparkSession.

---

## Para construir ETL's otimizados em PySpark:

- **Aproveite o formato de dados Parquet.**
- **Utilize caching e persist√™ncia para dados intermedi√°rios.**
- **Particione seus dados para otimizar leitura e escrita.**
- **Execute transforma√ß√µes como joins e agrega√ß√µes com opera√ß√µes distribu√≠das e broadcast quando necess√°rio.**

## Especifica√ß√£o T√©cnica

- **IDE:** VSCode (Visual Studio Code)
- **Linguagem:** Python
  - **Bibliotecas (principais):** pip install pyspark mysql-connector-python pandas

---

## üöÄ Configura√ß√£o e Execu√ß√£o

### 1. Criar o Ambiente Virtual

- **1.1. Abra o terminal ou o PowerShell no Windows.**

- **1.2. Navegue at√© o diret√≥rio base do projeto:**
  cd D:\GitHub\etl-pyspark

- **1.3. Crie o ambiente virtual:**
  python -m venv venv

- **1.4. Ative o ambiente virtual:**
  venv\Scripts\activate

### 2. Instalar as Depend√™ncias

- **2.1. Instale as depend√™ncias do arquivo requirements.txt existente**
  pip install -r requirements.txt

- **2.2. OU CASO exclua o arquivo requirements.txt existente:**
  - **2.2.1. Primeiro: Instale as depend√™ncias necess√°rias com pip:**
    pip install pandas beautifulsoup4 lxml selenium webdriver-manager
  - **2.2.2. Segundo: Gere arquivo requirements.txt**
    pip freeze > requirements.txt

### 3. Executar o Script

- **3.1. Navegue at√© o diret√≥rio onde est√° o script:**
  cd D:\GitHub\etl-pyspark

- **3.2. Execute o script com o Python:**
  python etl-otimizado.py

### 4. Manuten√ß√£o do Ambiente

- **4.1. Para Desativar o Ambiente:**
  **Quando terminar de usar o ambiente virtual, voc√™ pode desativ√°-lo com o comando:**
  deactivate

- **4.2. Para Reativar o Ambiente:**
  **Sempre que quiser executar novamente, reative o ambiente com:**
  venv\Scripts\activate
